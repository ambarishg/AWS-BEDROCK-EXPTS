{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"C:/Ambarish/NCERT/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "values_env = dotenv_values(\".env\")\n",
    "URL = values_env['URL']\n",
    "COLLECTION_NAME = values_env['COLLECTION_NAME']\n",
    "DIMENSION = int(values_env['DIMENSION'])\n",
    "MODEL_NAME = values_env['MODEL_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import qdrant_client as qc\n",
    "import qdrant_client.http.models as qmodels\n",
    "from qdrant_client.http.models import *\n",
    "\n",
    "import os\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the PDF file and return the text\n",
    "def get_pdf_data(file_path, num_pages = 1):\n",
    "    reader = PdfReader(file_path)\n",
    "    full_doc_text = \"\"\n",
    "    pages = reader.pages\n",
    "    num_pages = len(pages) \n",
    "    \n",
    "    try:\n",
    "        for page in range(num_pages):\n",
    "            current_page = reader.pages[page]\n",
    "            text = current_page.extract_text()\n",
    "            full_doc_text += text\n",
    "    except:\n",
    "        print(\"Error reading file\")\n",
    "    finally:\n",
    "        return full_doc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the text into chunks of chunk_length \n",
    "# [ default is 500] characters\n",
    "def get_chunks(fulltext:str,chunk_length =500) -> list:\n",
    "    text = fulltext\n",
    "\n",
    "    chunks = []\n",
    "    while len(text) > chunk_length:\n",
    "        last_period_index = text[:chunk_length].rfind('.')\n",
    "        if last_period_index == -1:\n",
    "            last_period_index = chunk_length\n",
    "        chunks.append(text[:last_period_index])\n",
    "        text = text[last_period_index+1:]\n",
    "    chunks.append(text)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = qc.QdrantClient(url=URL)\n",
    "METRIC = qmodels.Distance.COSINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:/Ambarish/NCERT/BIOTECH.pdf\n",
      "Full doc text length: 25081\n",
      "Full embeddings length: 60\n",
      "Inserting chunk 0 to 59\n",
      "Processing file: C:/Ambarish/NCERT/CHAP04-BIOLOGY-CLASS11.pdf\n",
      "Full doc text length: 31802\n",
      "Full embeddings length: 73\n",
      "Inserting chunk 0 to 72\n",
      "Processing file: C:/Ambarish/NCERT/HUMAN-WELFARE.pdf\n",
      "Full doc text length: 51060\n",
      "Full embeddings length: 121\n",
      "Inserting chunk 0 to 99\n",
      "Inserting chunk 100 to 120\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings for the chunks\n",
    "# Insert the chunks into the Qdrant collection\n",
    "# Insert the metadata for the chunks into the Qdrant collection\n",
    "FILES = os.listdir(FILE_PATH)\n",
    "FILES_FULL_PATH = [FILE_PATH + file for file in FILES]\n",
    "for filename in FILES_FULL_PATH:\n",
    "    print(f'Processing file: {filename}')\n",
    "    full_doc_text = get_pdf_data(filename)\n",
    "    print(f'Full doc text length: {len(full_doc_text)}')\n",
    "    payloads = []\n",
    "    li_id = []\n",
    "    corpus = []\n",
    "    Lines =get_chunks(full_doc_text,500)\n",
    "    for token in Lines:\n",
    "        corpus.append(token)\n",
    "        payloads.append({\"token\":token,\n",
    "                         \"filename\": os.path.basename(filename),\n",
    "                           \"type\":\"pdf\"})\n",
    "        li_id.append(str(uuid.uuid4()))\n",
    "    embeddings_all = model.encode(corpus, convert_to_tensor=True)\n",
    "    print(f'Full embeddings length: {len(embeddings_all)}')\n",
    "\n",
    "    CHUNK_SIZE = 100\n",
    "    for i in range(0, len(embeddings_all), CHUNK_SIZE):\n",
    "        if(i+CHUNK_SIZE > len(embeddings_all) -1):\n",
    "            new_chunk = len(embeddings_all) -1\n",
    "        else:\n",
    "            new_chunk = i+CHUNK_SIZE -1\n",
    "        print(\"Inserting chunk\", i , \"to\", new_chunk)\n",
    "        client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=qmodels.Batch(\n",
    "                ids = li_id[i:new_chunk],\n",
    "                vectors=embeddings_all[i:new_chunk].tolist(),\n",
    "                payloads=payloads[i:new_chunk]\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
